{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Neural Netwok imports\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding,LSTM,Bidirectional\n",
    "from keras.layers import Conv1D ,MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#text Processing import\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.tsv', sep='\\t', header=0)\n",
    "test_data  = pd.read_csv('test.tsv' , sep='\\t', header=0)\n",
    "phrase_train = train_data['Phrase'].values\n",
    "sentiments_data = train_data['Sentiment'].values\n",
    "num_labels = len(np.unique(sentiments_data))\n",
    "phrase_test  = test_data ['Phrase'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate stop word\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokonize phrases \"train,test\"\n",
    "train_data_clean = []\n",
    "test_data_clean = []\n",
    "# stem, tokenize sentiment train data\n",
    "for sentiment in phrase_train:\n",
    "    token = word_tokenize(sentiment)\n",
    "    sentiment_words = [stemmer.stem(word) for word in token if word not in stop_words]\n",
    "    train_data_clean.append(sentiment_words)\n",
    "# stem, tokenize sentiment test data\n",
    "for sentiment in phrase_test:\n",
    "    token = word_tokenize(sentiment)\n",
    "    sentiment_words = [stemmer.stem(word) for word in token if word not in stop_words]\n",
    "    test_data_clean.append(sentiment_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocab for all words (test,train)\n",
    "vocab = np.concatenate((train_data_clean,test_data_clean),axis = 0)\n",
    "vocab_dic = corpora.Dictionary(vocab)\n",
    "\n",
    "x_train = []\n",
    "x_train_seq_len = []\n",
    "x_test = []\n",
    "\n",
    "for sent in train_data_clean:\n",
    "    word_ids = [vocab_dic.token2id[word] for word in sent]\n",
    "    x_train.append(word_ids)\n",
    "    x_train_seq_len.append(len(word_ids))\n",
    "    \n",
    "for sent in test_data_clean:\n",
    "    word_ids = [vocab_dic.token2id[word] for word in sent]\n",
    "    x_test.append(word_ids)\n",
    "    x_train_seq_len.append(len(word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average sequence length\n",
    "seq_len = np.round((np.mean(x_train_seq_len) + 2*np.std(x_train_seq_len))).astype(int)\n",
    "# pad each sequence \"train,test\"\n",
    "x_train = sequence.pad_sequences(np.array(x_train), maxlen=seq_len)\n",
    "x_test = sequence.pad_sequences(np.array(x_test), maxlen=seq_len)\n",
    "# convert sentiment to categorical\n",
    "y_train = np_utils.to_categorical(sentiments_data, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model LSTM...\n"
     ]
    }
   ],
   "source": [
    "print 'Build model LSTM...'\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(len(vocab_dic.keys()), 128, input_length=seq_len))\n",
    "model_lstm.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(num_labels))\n",
    "model_lstm.add(Activation('softmax'))\n",
    "\n",
    "model_lstm.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model Bidirectional LSTM...\n"
     ]
    }
   ],
   "source": [
    "print 'Build model Bidirectional LSTM...'\n",
    "model_bi = Sequential()\n",
    "model_bi.add(Embedding(len(vocab_dic.keys()), 128, input_length=seq_len))\n",
    "model_bi.add(Bidirectional(LSTM(64)))\n",
    "model_bi.add(Dropout(0.5))\n",
    "model_bi.add(Dense(num_labels))\n",
    "model_bi.add(Activation('softmax'))\n",
    "\n",
    "model_bi.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model LSTM + CNN...\n"
     ]
    }
   ],
   "source": [
    "print 'Build model LSTM + CNN...'\n",
    "model_lstm_cnn = Sequential()\n",
    "model_lstm_cnn.add(Embedding(len(vocab_dic.keys()), 128, input_length=seq_len))\n",
    "model_lstm_cnn.add(Dropout(0.25))\n",
    "model_lstm_cnn.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model_lstm_cnn.add(MaxPooling1D(pool_size=4))\n",
    "model_lstm_cnn.add(LSTM(128))\n",
    "model_lstm_cnn.add(Dense(num_labels))\n",
    "model_lstm_cnn.add(Activation('softmax'))\n",
    "\n",
    "model_lstm_cnn.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_epoch = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train LSTM...\n",
      "Epoch 1/15\n",
      "156060/156060 [==============================] - 21s 136us/step - loss: 1.0038 - acc: 0.5969\n",
      "Epoch 2/15\n",
      "156060/156060 [==============================] - 15s 95us/step - loss: 0.8188 - acc: 0.6646\n",
      "Epoch 3/15\n",
      "156060/156060 [==============================] - 15s 95us/step - loss: 0.7656 - acc: 0.6823\n",
      "Epoch 4/15\n",
      "156060/156060 [==============================] - 15s 94us/step - loss: 0.7255 - acc: 0.6963\n",
      "Epoch 5/15\n",
      "156060/156060 [==============================] - 15s 96us/step - loss: 0.6927 - acc: 0.7071\n",
      "Epoch 6/15\n",
      "156060/156060 [==============================] - 15s 95us/step - loss: 0.6680 - acc: 0.7151\n",
      "Epoch 7/15\n",
      "156060/156060 [==============================] - 15s 95us/step - loss: 0.6477 - acc: 0.7232\n",
      "Epoch 8/15\n",
      "156060/156060 [==============================] - 15s 96us/step - loss: 0.6298 - acc: 0.7292\n",
      "Epoch 9/15\n",
      "156060/156060 [==============================] - 15s 95us/step - loss: 0.6147 - acc: 0.7329\n",
      "Epoch 10/15\n",
      "156060/156060 [==============================] - 15s 95us/step - loss: 0.6004 - acc: 0.7390\n",
      "Epoch 11/15\n",
      "156060/156060 [==============================] - 15s 95us/step - loss: 0.5876 - acc: 0.7430\n",
      "Epoch 12/15\n",
      "156060/156060 [==============================] - 15s 96us/step - loss: 0.5772 - acc: 0.7464\n",
      "Epoch 13/15\n",
      "156060/156060 [==============================] - 15s 96us/step - loss: 0.5662 - acc: 0.7502\n",
      "Epoch 14/15\n",
      "156060/156060 [==============================] - 15s 95us/step - loss: 0.5565 - acc: 0.7530\n",
      "Epoch 15/15\n",
      "156060/156060 [==============================] - 15s 94us/step - loss: 0.5464 - acc: 0.7571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4fa3853f10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'Train LSTM...'\n",
    "model_lstm.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Bidirectional...\n",
      "Epoch 1/15\n",
      "156060/156060 [==============================] - 21s 136us/step - loss: 1.0098 - acc: 0.5965\n",
      "Epoch 2/15\n",
      "156060/156060 [==============================] - 21s 132us/step - loss: 0.8262 - acc: 0.6623\n",
      "Epoch 3/15\n",
      "156060/156060 [==============================] - 21s 133us/step - loss: 0.7717 - acc: 0.6804\n",
      "Epoch 4/15\n",
      "156060/156060 [==============================] - 21s 132us/step - loss: 0.7325 - acc: 0.6944\n",
      "Epoch 5/15\n",
      "156060/156060 [==============================] - 21s 134us/step - loss: 0.7035 - acc: 0.7044\n",
      "Epoch 6/15\n",
      "156060/156060 [==============================] - 21s 133us/step - loss: 0.6798 - acc: 0.7115\n",
      "Epoch 7/15\n",
      "156060/156060 [==============================] - 21s 132us/step - loss: 0.6580 - acc: 0.7189\n",
      "Epoch 8/15\n",
      "156060/156060 [==============================] - 21s 134us/step - loss: 0.6388 - acc: 0.7248\n",
      "Epoch 9/15\n",
      "156060/156060 [==============================] - 21s 133us/step - loss: 0.6221 - acc: 0.7312\n",
      "Epoch 10/15\n",
      "156060/156060 [==============================] - 21s 133us/step - loss: 0.6064 - acc: 0.7368\n",
      "Epoch 11/15\n",
      "156060/156060 [==============================] - 21s 134us/step - loss: 0.5923 - acc: 0.7415\n",
      "Epoch 12/15\n",
      "156060/156060 [==============================] - 21s 132us/step - loss: 0.5793 - acc: 0.7461\n",
      "Epoch 13/15\n",
      "156060/156060 [==============================] - 21s 133us/step - loss: 0.5678 - acc: 0.7496\n",
      "Epoch 14/15\n",
      "156060/156060 [==============================] - 21s 134us/step - loss: 0.5566 - acc: 0.7540\n",
      "Epoch 15/15\n",
      "156060/156060 [==============================] - 21s 133us/step - loss: 0.5456 - acc: 0.7572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4fa0127c10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'Train Bidirectional...'\n",
    "model_bi.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Bidirectional...\n",
      "Epoch 1/15\n",
      "156060/156060 [==============================] - 9s 61us/step - loss: 1.1409 - acc: 0.5596\n",
      "Epoch 2/15\n",
      "156060/156060 [==============================] - 6s 40us/step - loss: 1.0231 - acc: 0.6090\n",
      "Epoch 3/15\n",
      "156060/156060 [==============================] - 6s 40us/step - loss: 0.9778 - acc: 0.6281\n",
      "Epoch 4/15\n",
      "156060/156060 [==============================] - 6s 40us/step - loss: 0.9473 - acc: 0.6397\n",
      "Epoch 5/15\n",
      "156060/156060 [==============================] - 6s 40us/step - loss: 0.9238 - acc: 0.6480\n",
      "Epoch 6/15\n",
      "156060/156060 [==============================] - 6s 41us/step - loss: 0.9052 - acc: 0.6544\n",
      "Epoch 7/15\n",
      "156060/156060 [==============================] - 6s 40us/step - loss: 0.8891 - acc: 0.6600\n",
      "Epoch 8/15\n",
      "156060/156060 [==============================] - 6s 40us/step - loss: 0.8764 - acc: 0.6648\n",
      "Epoch 9/15\n",
      "156060/156060 [==============================] - 6s 40us/step - loss: 0.8637 - acc: 0.6683\n",
      "Epoch 10/15\n",
      "156060/156060 [==============================] - 6s 40us/step - loss: 0.8544 - acc: 0.6705\n",
      "Epoch 11/15\n",
      "156060/156060 [==============================] - 6s 40us/step - loss: 0.8450 - acc: 0.6745\n",
      "Epoch 12/15\n",
      "156060/156060 [==============================] - 6s 40us/step - loss: 0.8362 - acc: 0.6760\n",
      "Epoch 13/15\n",
      "156060/156060 [==============================] - 6s 40us/step - loss: 0.8300 - acc: 0.6788\n",
      "Epoch 14/15\n",
      "156060/156060 [==============================] - 6s 41us/step - loss: 0.8236 - acc: 0.6806\n",
      "Epoch 15/15\n",
      "156060/156060 [==============================] - 6s 40us/step - loss: 0.8175 - acc: 0.6827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4f2aa09bd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'Train Bidirectional...'\n",
    "model_lstm_cnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Predict Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_lstm = model_lstm.predict(x_test)\n",
    "y_result_lstm = model_lstm.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_bi = model_lstm.predict(x_test)\n",
    "y_result_bi = model_lstm.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_lstm_cnn = model_lstm_cnn.predict(x_test)\n",
    "y_result_lstm_cnn = model_lstm.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 2s 27us/step\n",
      "0.618767441686 1.0\n"
     ]
    }
   ],
   "source": [
    "score, acc = model_lstm.evaluate(x_test, y_test_lstm, batch_size=batch_size)\n",
    "print \"{} {}\".format(score,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 3s 41us/step\n",
      "0.815549035388 0.812420805021\n"
     ]
    }
   ],
   "source": [
    "score, acc = model_bi.evaluate(x_test, y_test_bi, batch_size=batch_size)\n",
    "print \"{} {}\".format(score,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 1s 14us/step\n",
      "0.900319858494 1.0\n"
     ]
    }
   ],
   "source": [
    "score, acc = model_lstm_cnn.evaluate(x_test, y_test_lstm_cnn, batch_size=batch_size)\n",
    "print \"{} {}\".format(score,acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = pd.read_csv('test.tsv' , sep='\\t', header=0)\n",
    "test_data['Sentiment'] = y_result_lstm.reshape(-1,1) \n",
    "header = ['PhraseId', 'Sentiment']\n",
    "test_data.to_csv('lstm_sentiment.csv', columns=header, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = pd.read_csv('test.tsv' , sep='\\t', header=0)\n",
    "test_data['Sentiment'] = y_result_bi.reshape(-1,1) \n",
    "header = ['PhraseId', 'Sentiment']\n",
    "test_data.to_csv('Bi_sentiment.csv', columns=header, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = pd.read_csv('test.tsv' , sep='\\t', header=0)\n",
    "test_data['Sentiment'] = y_result_lstm_cnn.reshape(-1,1) \n",
    "header = ['PhraseId', 'Sentiment']\n",
    "test_data.to_csv('lstmCNN_sentiment.csv', columns=header, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = pd.read_csv('test.tsv' , sep='\\t', header=0)\n",
    "test_data.to_csv('test.csv', columns=header, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
